{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIfgQHS2SKE7yqA4ZFgMwd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitsiwach/CNN_basics/blob/main/Flower_classification_using_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WawVA54Fm7XE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "dom1jp4k_ZX9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# files.upload()"
      ],
      "metadata": {
        "id": "ijcLIbia_1xx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a kaggle folder\n",
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYk36MxqMRpF",
        "outputId": "b980e591-1488-4537-e0df-8cfe9b7bd601"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‚Äò/root/.kaggle‚Äô: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copying the Kaggle.json file to Kaggle folder\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "IzmDsi5IqdLh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# permission for the json to act\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "gJo5GlxyqskR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets list "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5w3bHJLrpat",
        "outputId": "c92bfdc0-878c-47e7-fb1e-88d082282cac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                             title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "meirnizri/covid19-dataset                                       COVID-19 Dataset                                      5MB  2022-11-13 15:47:17           2859         82  1.0              \n",
            "akshaydattatraykhare/diabetes-dataset                           Diabetes Dataset                                      9KB  2022-10-06 08:55:25          19239        539  1.0              \n",
            "thedevastator/jobs-dataset-from-glassdoor                       Salary Prediction                                     3MB  2022-11-16 13:52:31           2058         57  1.0              \n",
            "fuarresvij/gdp-growth-around-the-globe                          GDP Growth around the Globe                         122KB  2022-11-22 00:36:28            797         26  0.9411765        \n",
            "akshaydattatraykhare/data-for-admission-in-the-university       Data for Admission in the University                  4KB  2022-10-27 11:05:45           5793        134  1.0              \n",
            "arthurboari/taylor-swift-spotify-data                           Taylor Swift Spotify Data                            96KB  2022-11-21 13:46:01            550         23  0.9705882        \n",
            "dsfelix/us-stores-sales                                         US Stores Sales                                      84KB  2022-11-08 00:11:06           1345         25  1.0              \n",
            "thedevastator/fashion-products-on-amazon-ratings-prices-and-pa  Fashion Products on Amazon: Ratings, Prices, etc      8MB  2022-11-18 14:28:57            884         27  1.0              \n",
            "whenamancodes/predict-diabities                                 Predict Diabetes                                      9KB  2022-11-09 12:18:49           2955         63  1.0              \n",
            "prosperchuks/health-dataset                                     Diabetes, Hypertension and Stroke Prediction        597KB  2022-11-23 10:04:03            863         30  1.0              \n",
            "piterfm/fifa-football-world-cup                                 FIFA Football World Cup                              71KB  2022-11-28 09:44:12           1303         46  1.0              \n",
            "thedevastator/cancer-patients-and-air-pollution-a-new-link       Lung Cancer Prediction                               7KB  2022-11-14 13:40:40           1623         47  1.0              \n",
            "dbarteaux99/stable-diffusion-1-5                                Stable Diffusion 1.5 (normal and EMAonly) with vae    7GB  2022-10-23 15:40:29             62         17  0.9375           \n",
            "whenamancodes/credit-card-customers-prediction                  Credit Card Customers Prediction                    379KB  2022-10-30 13:03:27           3636         78  1.0              \n",
            "mukhazarahmad/worldwide-cancer-data                             Worldwide cancer data                                 2KB  2022-11-13 03:06:15            599         23  1.0              \n",
            "shilongzhuang/-women-clothing-ecommerce-sales-data              üè∑Ô∏èüëö Women Clothing Ecommerce Sales Data               5KB  2022-11-21 01:27:59            491         26  1.0              \n",
            "swaptr/layoffs-2022                                             Layoffs 2022                                         30KB  2022-11-28 21:06:41           1862         46  1.0              \n",
            "thedevastator/empowering-the-next-wave-of-entrepreneurs         US Startup companies over time (Timeseries)         974KB  2022-11-16 16:11:01            678         27  1.0              \n",
            "zvr842/global-pollution-by-counties                             Global pollution by counties                         15KB  2022-11-14 10:57:31           1332         29  0.9705882        \n",
            "dheerajmukati/india-gdp-19602022                                India GDP 1960-2022                                   1KB  2022-11-11 12:08:46           1166         33  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d alxmamaev/flowers-recognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exDEWPCNr0Dx",
        "outputId": "05c8d538-e0e5-4aba-9b54-85ecbc38f2f7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flowers-recognition.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip flowers-recognition.zip"
      ],
      "metadata": {
        "id": "--uR7sMzsDvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "N4RVLilCsYEl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = ImageDataGenerator(rescale=1/255,\n",
        "                              rotation_range=45,\n",
        "                              width_shift_range=0.2,\n",
        "                              height_shift_range=0.2,\n",
        "                              shear_range=0.2,\n",
        "                              zoom_range=0.2,\n",
        "                              fill_mode='nearest',\n",
        "                              horizontal_flip=True,\n",
        "                              vertical_flip=True,\n",
        "                              validation_split=0.25\n",
        "                             )\n",
        "\n",
        "test_gen = ImageDataGenerator(rescale = 1/255)"
      ],
      "metadata": {
        "id": "AZp-F-4FxKnt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_gen.flow_from_directory(directory=\"/content/flowers\",\n",
        "                                             target_size=(200,200),\n",
        "                                             batch_size=500,\n",
        "                                             class_mode=\"categorical\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL2ePzqG5Il9",
        "outputId": "b5738333-16c7-41b5-a14e-21ee53f55eea"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4317 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=32, kernel_size = (3,3), padding = \"same\", activation = \"relu\", input_shape = [200,200,3]))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation = \"relu\"))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(64, activation = \"relu\"))\n",
        "model.add(Dense(5, activation = \"softmax\"))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7H6Nsoa5isE",
        "outputId": "53e452f9-5182-48d2-88b8-189432cf5e98"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 200, 200, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 100, 100, 32)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 100, 100, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 50, 50, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 50, 50, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 25, 25, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 80000)             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               10240128  \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,341,957\n",
            "Trainable params: 10,341,957\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# early_stopping_cb = tensorflow.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True) NA as Validation data is not available\n",
        "\n",
        "model.fit(train_data,epochs=50,batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfhtnHeVau1J",
        "outputId": "ce3fa741-c39e-4023-9911-00025977eb13"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "9/9 [==============================] - 46s 5s/step - loss: 1.5770 - accuracy: 0.2814\n",
            "Epoch 2/50\n",
            "9/9 [==============================] - 46s 5s/step - loss: 1.3635 - accuracy: 0.3889\n",
            "Epoch 3/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 1.2650 - accuracy: 0.4494\n",
            "Epoch 4/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 1.1671 - accuracy: 0.4953\n",
            "Epoch 5/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 1.1166 - accuracy: 0.5270\n",
            "Epoch 6/50\n",
            "9/9 [==============================] - 46s 5s/step - loss: 1.0840 - accuracy: 0.5497\n",
            "Epoch 7/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 1.0584 - accuracy: 0.5659\n",
            "Epoch 8/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 1.0161 - accuracy: 0.6011\n",
            "Epoch 9/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.9852 - accuracy: 0.6074\n",
            "Epoch 10/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.9580 - accuracy: 0.6229\n",
            "Epoch 11/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.9480 - accuracy: 0.6273\n",
            "Epoch 12/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.9245 - accuracy: 0.6384\n",
            "Epoch 13/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.9069 - accuracy: 0.6437\n",
            "Epoch 14/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.8864 - accuracy: 0.6549\n",
            "Epoch 15/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.8717 - accuracy: 0.6602\n",
            "Epoch 16/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.8425 - accuracy: 0.6701\n",
            "Epoch 17/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.8306 - accuracy: 0.6771\n",
            "Epoch 18/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.8305 - accuracy: 0.6796\n",
            "Epoch 19/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.8015 - accuracy: 0.6852\n",
            "Epoch 20/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.7982 - accuracy: 0.6875\n",
            "Epoch 21/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.7915 - accuracy: 0.6833\n",
            "Epoch 22/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.7847 - accuracy: 0.6970\n",
            "Epoch 23/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.7696 - accuracy: 0.7005\n",
            "Epoch 24/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.7541 - accuracy: 0.7056\n",
            "Epoch 25/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.7472 - accuracy: 0.7095\n",
            "Epoch 26/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.7298 - accuracy: 0.7153\n",
            "Epoch 27/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.7295 - accuracy: 0.7116\n",
            "Epoch 28/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.7113 - accuracy: 0.7243\n",
            "Epoch 29/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.7077 - accuracy: 0.7255\n",
            "Epoch 30/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.7051 - accuracy: 0.7260\n",
            "Epoch 31/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.6903 - accuracy: 0.7311\n",
            "Epoch 32/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.6753 - accuracy: 0.7445\n",
            "Epoch 33/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.6639 - accuracy: 0.7470\n",
            "Epoch 34/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.6866 - accuracy: 0.7359\n",
            "Epoch 35/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.6618 - accuracy: 0.7519\n",
            "Epoch 36/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.6857 - accuracy: 0.7403\n",
            "Epoch 37/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.6600 - accuracy: 0.7473\n",
            "Epoch 38/50\n",
            "9/9 [==============================] - 43s 5s/step - loss: 0.6337 - accuracy: 0.7459\n",
            "Epoch 39/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.6384 - accuracy: 0.7540\n",
            "Epoch 40/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.6153 - accuracy: 0.7644\n",
            "Epoch 41/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.6106 - accuracy: 0.7640\n",
            "Epoch 42/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.6139 - accuracy: 0.7697\n",
            "Epoch 43/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.6217 - accuracy: 0.7663\n",
            "Epoch 44/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.6363 - accuracy: 0.7586\n",
            "Epoch 45/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.6197 - accuracy: 0.7637\n",
            "Epoch 46/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.6016 - accuracy: 0.7695\n",
            "Epoch 47/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.6082 - accuracy: 0.7677\n",
            "Epoch 48/50\n",
            "9/9 [==============================] - 43s 5s/step - loss: 0.5895 - accuracy: 0.7702\n",
            "Epoch 49/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.5803 - accuracy: 0.7804\n",
            "Epoch 50/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.5731 - accuracy: 0.7806\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9c7c4b7a10>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ep2hxiLtmS_z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}